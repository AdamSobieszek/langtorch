{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installation\n",
        "\n",
        "LangTorch works with Python 3.8 or higher. To install LangTorch using pip:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "d0b2f2f0d1134b23"
      },
      "id": "d0b2f2f0d1134b23"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install langtorch"
      ],
      "metadata": {
        "id": "22a61b1b1221f143"
      },
      "id": "22a61b1b1221f143"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To us the OpenAI API, you need to set the `OPENAI_API_KEY` environment variable. You can find your API key in the OpenAI dashboard."
      ],
      "metadata": {
        "collapsed": false,
        "id": "4e001fb486dc11"
      },
      "id": "4e001fb486dc11"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\""
      ],
      "metadata": {
        "id": "e7ec525bcfd71e06"
      },
      "id": "e7ec525bcfd71e06"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Text Tensors and Simple chains\n",
        "\n",
        "### Getting tensor text data\n",
        "\n",
        "`TextTensors` are designed to simplify simultaneous handling of text data. They inherit most functionality from PyTorch's `torch.Tensor` but hold textual data, which you may create from documents, prompt templates, completion dictionaries and more."
      ],
      "metadata": {
        "collapsed": false,
        "id": "f28f0fe0532da2ad"
      },
      "id": "f28f0fe0532da2ad"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompts (shape=(2, 1)):\n",
            "[[This is the first prompt, with field to fill in.]\n",
            " [This is another prompt, also with a field.]],\n",
            "\n",
            "completion := TextTensor([ data ], shape=torch.Size([1]))\n",
            "\n",
            "two_completions := TextTensor([ data   value ], shape=torch.Size([2]))\n"
          ]
        }
      ],
      "source": [
        "from langtorch import TextTensor\n",
        "\n",
        "prompts = TextTensor([[\"This is the first prompt, with {field} to fill in.\"],\n",
        "                                      [\"This is another prompt, also with a {field}.\"   ]])\n",
        "print(f\"prompts (shape={tuple(prompts.shape)}):\\n{prompts},\\n\")\n",
        "\n",
        "completion = TextTensor([{\"field\": \"data\"}])\n",
        "print(f\"completion (shape={tuple(completion)}):\\n{completion}\\n\")\n",
        "\n",
        "two_completions = TextTensor([{\"field\": \"data\"}, {\"field\": \"value\"}])\n",
        "print(f\"two_completions (shape={tuple(two_completions.shape)}):\\n{two_completions})\\n\")"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T12:22:38.122728900Z",
          "start_time": "2024-04-12T12:22:38.110551500Z"
        },
        "id": "cd3ef71290cefa40",
        "outputId": "60d70432-b4aa-4982-9ef8-20c36d874060"
      },
      "id": "cd3ef71290cefa40"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### TextTensor operations for prompt templating\n",
        "\n",
        "`TextTensors` provide operations that allow for formatting and editing many entries at the same time according to array broadcasting rules. Adding a `TextTensors` appends its content to the other, while multiplication performs a more complex operation that can be used for template formatting:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "d4a6ad810df89bea"
      },
      "id": "d4a6ad810df89bea"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "lines = TextTensor(open('paper.md','r').readlines())\n",
        "lines = lines + \" Have a great day!\"\n",
        "lines"
      ],
      "metadata": {
        "id": "48058458877d2fe4"
      },
      "id": "48058458877d2fe4"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "collapsed": false,
        "id": "c371a5940f0ea1e6"
      },
      "id": "c371a5940f0ea1e6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "completions = TextTensor([[{\"name\": \"Luciano\"}],\n",
        "                          [{\"name\": \"Massimo\"}]])\n",
        "print(completions)"
      ],
      "metadata": {
        "id": "ab0e67cadd133177"
      },
      "id": "ab0e67cadd133177"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "prompts = TextTensor([\"Hello, {name}!\"]) * completions\n"
      ],
      "metadata": {
        "id": "f493eadbe53fccea"
      },
      "id": "f493eadbe53fccea"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A useful informal definition for the multiplication operation is that when two entries are multiplied, the right Text acts like a format operation: replacing keys with values (here, {name} with Luciano) or appending if there is nothing to replace. For a more in depth look, see [TextTensor Multiplication](langtorch.org/reference/multiplication).\n",
        "### Performing a task with TextModules\n",
        "\n",
        "\n",
        "TextModules like `nn.Module` implement a forward method that works on (text) tensors. By default, they can be initialized by passing a TextTensor of prompts, that in the forward pass will be formatted using the input TextTensor (just like in the example above).  \n",
        "\n",
        "To achieve interesting behavior, an nn.Module layer usually ends with passing the multiplied tensors to an \"activation function\". By analogy, TextModules usually end with an activation of an LLM call on the formatted prompts (for more on this parallel see. [langtorch.tt](langtorch.org/reference/tt)). LangTorch activation like `OpenAI` execute their LLM calls on each entry of the input TextTensor in parallel."
      ],
      "metadata": {
        "collapsed": false,
        "id": "9efb529bf4c6a20c"
      },
      "id": "9efb529bf4c6a20c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from langtorch import TextModule, OpenAI\n",
        "llm = OpenAI(\"gpt4\", T = 0.) # Pass any API kwargs here to customize the call\n",
        "translate = TextModule(\"Translate this text to Polish: {}\", activation=llm)\n"
      ],
      "metadata": {
        "id": "a4f6302a75b7fb7e"
      },
      "id": "a4f6302a75b7fb7e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "output = translate(prompts)\n"
      ],
      "metadata": {
        "id": "8c2ca1282130cfd2"
      },
      "id": "8c2ca1282130cfd2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Implementing popular methods\n",
        "### Parallel and Chained calls with TextModules\n",
        "LangTorch uses a custom implementation to speed up and cache api calls, that by default run in parallel for all TextTensor entries passed to an LLM activation. As such, running calls in parallel is done automatically if either multiple prompts, multiple input values or both are passed to an LLM.\n",
        "The simplest way to chain TextModule is to directly use `torch.nn.Sequential`. To create any complex chain you may, as in torch, define a module subclass that adds custom behavior or combines many submodules in one.  We will show these on examples of popular LLM methods.\n",
        "\n",
        "### Chain of Thought\n",
        "The simplest example of a custom module are those that implement prompting methods like Chain of Thought, where all we need is to append a fixed string to the input. This can be done by creating a reusable  TextModule that we can chain with any task module. Let's define a module with a task prompt template:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "de2f3749a8a6dbbf"
      },
      "id": "de2f3749a8a6dbbf"
    },
    {
      "cell_type": "code",
      "source": [
        "some_prompt_template = \"Solve this equation: {}\\n\"\n",
        "task = TextModule(some_prompt_template)\n",
        "print(task(TextTensor(\"2+2 =\")))"
      ],
      "metadata": {
        "id": "ja-yEF8xY9G7"
      },
      "id": "ja-yEF8xY9G7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`{}` in a prompt template is a positional argument, that accepts one entry from the input. We can have multiple such placeholders in one prompt, if the input consists of many entries, e.g. `input = TextTensor([{\"key1\":\"text1\", \"key2\":\"text2\"}])`.\n",
        "\n",
        "For our chain of thought module we should use the placeholder `{*}`, which is a \"wildcard\" key that places all of the input entries in it's place (here, the \"Solve this equation\" prompt and its completion)."
      ],
      "metadata": {
        "id": "u22Fx-UXZQV1"
      },
      "id": "u22Fx-UXZQV1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "chain_of_thought = TextModule(\"{*} Let's think step by step.\")\n",
        "\n",
        "task_module_w_CoT = torch.nn.Sequential(\n",
        "    task,\n",
        "    chain_of_thought,\n",
        "    OpenAI(\"gpt-3.5-turbo\")  # We end with an OpenAI model call. We could omit the model name as this is the default OpenAI model\n",
        ")\n",
        "\n",
        "input_tensor = TextTensor([\"170*32 =\", \"4*20 =\", \"123*45/10 =\", \"2**10*5 =\"])\n",
        "output_tensor = chain(input_tensor)\n",
        "output_tensor"
      ],
      "metadata": {
        "id": "ae1d705bce57772b"
      },
      "id": "ae1d705bce57772b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in PyTorch, we can also create a class that can implement a `forward` and `__init__` methods."
      ],
      "metadata": {
        "id": "W6x08lVbbSTG"
      },
      "id": "W6x08lVbbSTG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class ChainOfThought(TextModule):\n",
        "    def forward(self, input):\n",
        "        return super()(input + \" Let's think step by step.\")\n",
        "\n",
        "task = ChainOfThought(some_prompt_template, activation=llm)\n"
      ],
      "metadata": {
        "id": "88bb491fe97fc08b"
      },
      "id": "88bb491fe97fc08b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble / Self-consistency\n",
        "Many benefits of being able to represent texts \"geometrically\" in a matrix / tensor comes from being able to create a meaningful structure, where e.g. a 2d matrix has columns representing different versions of the same text and subsequent entries represent subsequent paragraphs. Methods like ensemble voting and self-consistency require creating multiple completions for the same task, which can be representing by adding such a \"version\" dimension.\n",
        "In this example, we will build such a module that for each entry creates multiple answer entries and combines them back together to increase the overall performance. First, to add a new dimension with different answers given by the LLM we need only adjust the `n` parameter. Additionally, we can set the system message:\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "a50fbf577de921ae"
      },
      "id": "a50fbf577de921ae"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from langtorch import OpenAI\n",
        "ensemble_llm = OpenAI(\"gpt-3.5-turbo\",\n",
        "                      system_message=\"You are a rewriting bot that answers only with the revised text\",\n",
        "                      T=1.1, # High temperature to sample diverse completions\n",
        "                      n = 5) # 5 completions for each entry\n"
      ],
      "metadata": {
        "id": "2f3948f6860537aa"
      },
      "id": "2f3948f6860537aa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use a concrete example, we will write a module that uses an ensemble to compress a text paragraph by paragraph. The task description is inspired by the Chain of Density method. For now, let's assume `paragraphs` is defined as a TextTensor with 15 paragraph-entries:\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "6ef0a3d76b73bf95"
      },
      "id": "6ef0a3d76b73bf95"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "rewrite = TextModule([\"Compress all information from the paragraph into an entity-dense telegraphic summary: \"], activation = ensemble_llm)\n",
        "ensemble_summaries = rewrite(paragraphs)\n",
        "print(ensemble_summaries.shape)\n"
      ],
      "metadata": {
        "id": "d71e9832f82ee901"
      },
      "id": "d71e9832f82ee901"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "combined_summaries = langtorch.mean(ensemble_summaries, dim=-1)\n",
        "print(combined_summaries.shape)\n"
      ],
      "metadata": {
        "id": "59cf831333e88cf8"
      },
      "id": "59cf831333e88cf8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "summary = langtorch.mean(combined_summaries, dim=-1)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "1d9f1c3fd3b1700"
      },
      "id": "1d9f1c3fd3b1700"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar approaches can be used for more complicated ensemble methods or combined with methods like chain of thought to increase accuracy with \"self-consistency\".\n",
        "\n",
        "### Working with structured documents\n",
        " Instead of strings, each entry of a TextTensor is an instance of [`langtorch.Text`](reference/text), which allows for more complex text processing. The `Text` class can load documents, parse most markup languages and provide a helpful interface for accessing and modifying their structured text segments. We will prepare data for a rewrite task like before by parsing a markdown file of a paper on the abilities of language models, available here [paper.md](/static/paper.txt){:download=\"paper.md\"}. As the text has headers and other text blocks, we'll to select only paragraphs, which can be done with `iloc` and `loc` accessors:  \n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "4ea191cb538d863e"
      },
      "id": "4ea191cb538d863e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/yourusername/langtorch/main/paper.md"
      ],
      "metadata": {
        "id": "514259b30537e1a0"
      },
      "id": "514259b30537e1a0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from langtorch import Text\n",
        "paper = Text.from_file(\"paper.md\")\n",
        "first_block = paper.iloc[0]\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-13T15:58:20.896611100Z",
          "start_time": "2024-04-13T15:58:08.056028700Z"
        },
        "id": "71fc197b330f15cb"
      },
      "id": "71fc197b330f15cb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(set(paper.keys()))\n"
      ],
      "metadata": {
        "id": "e8908fb47464e513"
      },
      "id": "e8908fb47464e513"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "paragraphs = paper.loc[\"Para\"]\n",
        "rewritten_paragraphs = rewrite(paragraphs)\n",
        "paper.loc[\"Para\"] = rewritten_paragraphs\n",
        "print(paper)\n"
      ],
      "metadata": {
        "id": "87559ddd63f62aa1"
      },
      "id": "87559ddd63f62aa1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Using Tensor Embeddings to Build Retrievers\n",
        "Using embeddings with TextTensors is extremely easy, as every TextTensor can generate its own embedding, as well as know to automatically act as if it was an embedding tensor when passed to torch functions like cosine similarity. These representations (available under the `.embedding` attribute) are moreover automatically created only right before they are needed (via a set embedding model, by default OpenAI's `text-embedding-3-small`).\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "4c6720b07213067c"
      },
      "id": "4c6720b07213067c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "tensor1 = TextTensor([[[\"Yes\"], [\"No\"]]])\n",
        "tensor2 = TextTensor([\"Yeah\", \"Nope\", \"Yup\", \"Non\"])\n",
        "\n",
        "torch.cosine_similarity(tensor1,tensor2)\n"
      ],
      "metadata": {
        "id": "8471c41a02c90cab"
      },
      "id": "8471c41a02c90cab"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Custom Retriever and RAG modules\n",
        "Using how `TextTensor`s can automatically act as a`Tensor` of it's embeddings, we can very compactly implement e.g. a retriever, which for each entry in the input finds in parallel `k` entries with the highest cosine similarity among the documents it holds:\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "55f9b5573924ac01"
      },
      "id": "55f9b5573924ac01"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class Retriever(TextModule):\n",
        "    def __init__(self, documents: TextTensor):\n",
        "        super().__init__()\n",
        "        self.documents = TextTensor(documents).view(-1)\n",
        "\n",
        "    def forward(self, query: TextTensor, k: int = 5):\n",
        "        cos_sim = torch.cosine_similarity(self.documents, query.reshape(1))\n",
        "        return self.documents[cos_sim.topk(k)]\n"
      ],
      "metadata": {
        "id": "7aa72db3e92792c"
      },
      "id": "7aa72db3e92792c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "retriever = Retriever(open(\"doc.txt\", \"r\").readlines())\n",
        "query = TextTensor(\"How to build a retriever?\")\n",
        "print(retriever(query))\n"
      ],
      "metadata": {
        "id": "6d9c02c232d18073"
      },
      "id": "6d9c02c232d18073"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how the implementation didn't require us to learn about any new operations we would not find in regular PyTorch. One goal of LangTorch is to give developers control over these lower level operations, while being able to write compact code without a multitude of classes. For this reason implementations such as the retriever above are not pre-defined classes in the main package.  \n",
        "We can now compose this module with a Module making LLM calls to get a custom Retrieval Augmented Generation pipeline:\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "898a81d71b708804"
      },
      "id": "898a81d71b708804"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class RAG(TextModule):\n",
        "    def __init__(self, documents: TextTensor, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.retriever = Retriever(documents)\n",
        "\n",
        "    def forward(self, user_message: TextTensor, k: int = 5):\n",
        "        retrieved_context = self.retriever(user_message, k) +\"\\n\"\n",
        "        user_message = user_message + \"\\nCONTEXT:\\n\" + retrieved_context.sum()\n",
        "        return super().forward(user_message)\n"
      ],
      "metadata": {
        "id": "b84b8f16e7a7602"
      },
      "id": "b84b8f16e7a7602"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "rag_chat = RAG(paragraphs,\n",
        "               prompt=\"Use the context to answer the following user query: \",\n",
        "               activation=\"gpt-3.5-turbo\")\n",
        "assistant_response = rag_chat(user_query)\n"
      ],
      "metadata": {
        "id": "9710c48b15e86dd8"
      },
      "id": "9710c48b15e86dd8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "With only small modifications to the retriever this module could also perform batched inference — performing multiple simultaneous queries without much additional latency. Note, `prompt` and `activation` are arguments inherited from TextModule and need the `super().forward` call to work.\n",
        "We are excited to see what you will build with LangTorch. If you want to share some examples or have any questions, feel free to ask on our [discord](https://discord.gg/jkreqtCCkv). In the likely event of encountering a bug send it on discord or post on the [GitHub Repo](https://github.com/AdamSobieszek/langtorch) and we will fix it ASAP."
      ],
      "metadata": {
        "collapsed": false,
        "id": "2b0b68131396bd40"
      },
      "id": "2b0b68131396bd40"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
