{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LangTorch\n",
    "\n",
    "To install LangTorch using pip:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "d0b2f2f0d1134b23"
   },
   "id": "d0b2f2f0d1134b23"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Setup Notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Install LangTorch\n",
    "!pip install langtorch"
   ],
   "metadata": {
    "id": "22a61b1b1221f143",
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-05T14:41:40.820987100Z",
     "start_time": "2024-06-05T14:41:36.535584400Z"
    }
   },
   "id": "22a61b1b1221f143"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To use the OpenAI API as our LLM, we need to set the `OPENAI_API_KEY` environment variable. You can find your API key on platform.openai.com"
   ],
   "metadata": {
    "collapsed": false,
    "id": "4e001fb486dc11"
   },
   "id": "4e001fb486dc11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\"  # Replace with your actual OpenAI API key"
   ],
   "metadata": {
    "id": "e7ec525bcfd71e06"
   },
   "id": "e7ec525bcfd71e06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Perform multiple LLM calls with TextTensors"
   ],
   "metadata": {
    "collapsed": false,
    "id": "f28f0fe0532da2ad"
   },
   "id": "f28f0fe0532da2ad"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from langtorch import TextTensor  # holds texts instead of weights, supports tensor operations\n",
    "from langtorch import TextModule  # torch.nn modules working on TextTensors, perform prompt templating and llm calls"
   ],
   "metadata": {
    "id": "3dff6245c28f56c7",
    "ExecuteTime": {
     "end_time": "2024-06-05T14:41:44.200187800Z",
     "start_time": "2024-06-05T14:41:41.059153800Z"
    }
   },
   "id": "3dff6245c28f56c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**`TextTensors`** are designed to streamline working with many pieces of text and performing parallel LLM calls. `langtorch.TextTensor` is a subclass of PyTorch's `torch.Tensor` that:\n",
    "\n",
    "- **Holds text entries** instead of numerical weights.\n",
    "- **Special Structure:** `TextTensors` entries can represent chunked documents, prompt templates, completion dictionaries, chat histories, and more.\n",
    "- **Represents Geometrically:** `TextTensors` have a shape and can be modified with PyTorch functions (reshape, stack, etc.).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In this example, we will create tensors holding prompt templates, fill them with a tensor of completion dictionaries, and send them to the OpenAI API.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "39dcc0595a6a376d"
   },
   "id": "39dcc0595a6a376d"
  },
  {
   "cell_type": "code",
   "source": [
    "prompt_tensor = TextTensor([[\"Is this an email address? {input_field}\"],\n",
    "                            [\"Is this a valid web link? {input_field}\"]])\n",
    "\n",
    "# Adding TextTensors appends their content according to broadcasting rules\n",
    "prompt_tensor += \" (Answer 'Yes' or 'No')\"\n",
    "print(prompt_tensor)\n",
    "print(\"Shape =\", prompt_tensor.shape)"
   ],
   "metadata": {
    "id": "PSYLm9qT4PGj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "78676ca0-7a9e-4b0e-af17-6be0b6378ae8",
    "ExecuteTime": {
     "end_time": "2024-06-05T14:41:44.262001800Z",
     "start_time": "2024-06-05T14:41:44.193223900Z"
    }
   },
   "id": "PSYLm9qT4PGj",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Is this an email address? input_field (Answer 'Yes' or 'No')]\n",
      " [Is this a valid web link? input_field (Answer 'Yes' or 'No')]]\n",
      "Shape = torch.Size([2, 1])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "`TextModules` are `torch.nn.Modules` that work on `TextTensors`:\n",
    "\n",
    "- **Tensor of Prompts:** They hold a tensor of prompts instead of numerical weights.\n",
    "- **Input Handling:** They accept `TextTensors` as input, which are used to format the prompt tensor.\n",
    "- **Formatting and Broadcasting:** This allows formatting multiple prompts on multiple completions, controlling which prompt gets which input through broadcasting rules.\n",
    "- **Activation Function:** Most torch layers end with an *activation function*. Similarly, `TextModules` end in an *activation* of an LLM call.\n",
    "\n",
    "In this example, we will create a `TextModule` that ends in a call to an OpenAI model. **This module can now execute both tasks in parallel on as many inputs as we'd like:**\n"
   ],
   "metadata": {
    "id": "YtZDQxJ89xFC"
   },
   "id": "YtZDQxJ89xFC"
  },
  {
   "cell_type": "code",
   "source": [
    "tasks_module = TextModule(prompt_tensor, activation=\"gpt-3.5-turbo\")\n",
    "\n",
    "input_completions = TextTensor([{\"input_field\": \"contact@langtorch.org\"}, {\"input_field\": \"https://langtorch.org\" }])\n",
    "\n",
    "# The first row of the output are answers to \"Is this an email address?\", second to \"Is this a valid web link?\"\n",
    "# Columns are the two input completions\n",
    "print(tasks_module(input_completions))"
   ],
   "metadata": {
    "id": "jHwDZ4GX46hK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7f781a72-1e53-444e-ebbf-325b6df1e8c1",
    "ExecuteTime": {
     "end_time": "2024-06-05T14:41:56.882897200Z",
     "start_time": "2024-06-05T14:41:44.258002400Z"
    }
   },
   "id": "jHwDZ4GX46hK",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Yes   No ]\n",
      " [No    Yes]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparison with the OpenAI Package\n",
    "\n",
    "The `TextModule` above both formats the prompts and sends them to the OpenAI activation (`langtorch.OpenAI`). Let's compare LangTorch to the OpenAI package.\n",
    "\n",
    "First, we'll separate the formatting and API steps.\n",
    "\n",
    "> A core feature of `TextTensors` is that they allow us to easily format several prompts on several inputs.\n",
    ">\n",
    "> LangTorch achieves this by **defining the product** of two `TextTensors`: `text1*text2` as an operation akin to `text1.format(**text2)`. As shown below this is what happens in a `TextModule` before adding an activation:\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "5de346885d33eb67"
   },
   "id": "5de346885d33eb67"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Is this an email address? contact@langtorch.org (Answer 'Yes' or 'No')   Is this an email address? https://langtorch.org (Answer 'Yes' or 'No')]\n",
      " [Is this a valid web link? contact@langtorch.org (Answer 'Yes' or 'No')   Is this a valid web link? https://langtorch.org (Answer 'Yes' or 'No')]]\n"
     ]
    }
   ],
   "source": [
    "# Using TextModule\n",
    "tasks_module = TextModule(prompt_tensor)\n",
    "prompts = tasks_module(input_completions)\n",
    "\n",
    "# Equivalently, using \"TextTensor multiplication\"\n",
    "prompts = prompt_tensor*input_completions\n",
    "print(prompts)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1bd9231ac309e8c",
    "outputId": "772732d5-1b40-48b3-9a2f-f42c71631888",
    "ExecuteTime": {
     "end_time": "2024-06-05T14:41:56.936840900Z",
     "start_time": "2024-06-05T14:41:56.880380500Z"
    }
   },
   "id": "c1bd9231ac309e8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    " The code above introduces the multiplication operation (used in TextModules), which acts like a more powerful format operation and allows for the various features of TextTensors. For a more in depth look, see [TextTensor Multiplication](langtorch.org/reference/multiplication).\n",
    "\n",
    "We can send the formatted prompts to the OpenAI API by creating a `langtorch.OpenAI` module (the \"activation\") and compare speed between three API use cases:\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "cc6f82ff60c73d4a"
   },
   "id": "cc6f82ff60c73d4a"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "[ChatCompletion(id='chatcmpl-9Wm7rXiwaMHZCAJGykdKKfilYhVvH', choices=[Choice(finish_reason='length', index=0, message=ChatCom...\n",
      " OpenAI loop time taken: 2.56 seconds\n",
      "2.\n",
      "[[Yes   No ]\n",
      " [No    Yes]]\n",
      " LangTorch time taken: 0.77 seconds\n",
      "3.\n",
      "[[Yes   No ]\n",
      " [No    Yes]]\n",
      " LangTorch on repeated requests time taken: 0.02 seconds\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import langtorch\n",
    "import time\n",
    "\n",
    "langtorch_api = langtorch.OpenAI(\"gpt-3.5-turbo\", system_message=\"You are a helpful assistant.\", max_token=1, T=0.)\n",
    "openai_api = openai.OpenAI()\n",
    "\n",
    "# Open AI package\n",
    "start = time.time()\n",
    "responses = []\n",
    "for prompt in prompts.flat:\n",
    "    responses.append(openai_api.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=1,\n",
    "        temperature=0.\n",
    "    ))\n",
    "print(f\"1.\\n{str(responses)[:125]}...\")\n",
    "print(f\" OpenAI loop time taken: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# LangTorch\n",
    "start = time.time()\n",
    "responses = langtorch_api(prompts)\n",
    "print(f\"2.\\n{responses}\")\n",
    "print(f\" LangTorch time taken: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# LangTorch on repeated requests\n",
    "start = time.time()\n",
    "responses = langtorch_api(prompts)\n",
    "print(f\"3.\\n{responses}\")\n",
    "print(f\" LangTorch on repeated requests time taken: {time.time() - start:.2f} seconds\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90f08316c82c7e11",
    "outputId": "2bf36d2f-0843-4172-e464-1ae84c72f39b",
    "ExecuteTime": {
     "end_time": "2024-06-05T14:42:00.346529100Z",
     "start_time": "2024-06-05T14:41:56.935840900Z"
    }
   },
   "id": "90f08316c82c7e11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "The OpenAI Activation in LangTorch (`langtorch.OpenAI`) isn't just a wrapper around the OpenAI package. The observed speed up comes from the fact that the LangTorch implementation:\n",
    "\n",
    "- Sends API calls in parallel, allowing multiple completions to be generated much faster than calling the OpenAI chat completion endpoint in sequence.\n",
    "- Saves on tokens and speeds up subsequent calls by caching API results, especially for embeddings and when the temperature is set to zero.\n",
    "- Optimizes requested API calls removing duplicates\n",
    "\n",
    "`langtorch.OpenAI` also:\n",
    "\n",
    "- Operates directly on `TextTensors`, returning calls in the same shape as the input.\n",
    "- Handles API errors and retries by default"
   ],
   "metadata": {
    "collapsed": false,
    "id": "fcd90c1e3545e667"
   },
   "id": "fcd90c1e3545e667"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Implementing popular methods\n",
    "\n",
    "### Chained Calls and Simple Zero-Shot Chain of Thought\n",
    "\n",
    "LangTorch integrates seamlessly with torch, allowing you to easily chain `TextModules` using `torch.nn.Sequential`. This can be used to chain multiple LLM calls or additional prompting methods. A simplified example is a zero-shot Chain of Thought, for which we can create a reusable `TextModule`:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "de2f3749a8a6dbbf"
   },
   "id": "de2f3749a8a6dbbf"
  },
  {
   "cell_type": "code",
   "source": [
    "CoT = TextModule(\"{*} Let's think step by step.\")"
   ],
   "metadata": {
    "id": "ja-yEF8xY9G7",
    "ExecuteTime": {
     "end_time": "2024-06-05T14:42:00.397670400Z",
     "start_time": "2024-06-05T14:42:00.344528Z"
    }
   },
   "id": "ja-yEF8xY9G7",
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "`{}` in a prompt template is a positional argument, taking one input argument in each entry. For our chain of thought module we use the placeholder `{*}`, which is a \"wildcard\" key that places all the input entries in its place.\n",
    "\n",
    "Now to chain these `torch.nn.Sequential`:"
   ],
   "metadata": {
    "id": "u22Fx-UXZQV1"
   },
   "id": "u22Fx-UXZQV1"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[[To calculate 170 multiplied by 32, you can do↵   1. Multiply 123 * 45 = 5535     First, we need to solve the exponential term,↵\n   the following steps:                            2. Divide 5535 by 10 = 553.5     2**10, which means 2 to the power of 10. Thi↵ \n                                                                                   s equals 1024.                                 \n  1. Multiply 170 by 2: 170 * 2 = 340              Therefore, 123*45/10 = 553.5.                                                  \n  2. Multiply 340 by 10 (since 32 is 10 times l↵                                   Now we can multiply 1024 by 5.                 \n  arger than 2): 340 * 10 = 3400                                                                                                  \n  3. Add the two results together: 340 + 3400 =↵                                   1024 * 5 = 5120                                \n   3740                                                                                                                           \n                                                                                   Therefore, 2**10*5 = 5120.                     \n  Therefore, 170 multiplied by 32 equals 3740.                                                                                   ]]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "calculate = TextModule(\"Calculate the following: {} = ?\")\n",
    "\n",
    "calculate_w_CoT = torch.nn.Sequential(\n",
    "    calculate,\n",
    "    CoT,\n",
    "    langtorch.OpenAI(\"gpt-3.5-turbo\") ,\n",
    "    # You can add sequential calls here\n",
    ")\n",
    "\n",
    "input_tensor = TextTensor([\"170*32\", \"123*45/10\", \"2**10*5\"])\n",
    "output_tensor = calculate_w_CoT(input_tensor)\n",
    "output_tensor.view(1,-1) # We use torch methods to reshape TextTensors and view entries in columns"
   ],
   "metadata": {
    "id": "ae1d705bce57772b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e889b758-ab47-4dc9-adc1-1138fa671d92",
    "ExecuteTime": {
     "end_time": "2024-06-05T14:42:02.796073500Z",
     "start_time": "2024-06-05T14:42:00.402172600Z"
    }
   },
   "id": "ae1d705bce57772b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ensemble / Self-Consistency\n",
    "\n",
    "Representing texts geometrically in a matrix or tensor allows for creating meaningful structures. Methods like ensemble voting and self-consistency involve generating multiple completions for the same task, easily represented by adding a dimension.\n",
    "\n",
    "In this example, we build a module that creates multiple Chain-of-Thought answers for each input. These create separate `TextTensor` entries that we combine using a \"linear layer\" to marginalize over them, improving overall performance (see [Wang et al., 2022](https://arxiv.org/abs/2203.11171)).\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "4ea191cb538d863e"
   },
   "id": "4ea191cb538d863e"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "calculate = TextModule(\"Calculate the following: {} = ? Let's think step by step.\")\n",
    "\n",
    "ensemble_llm = langtorch.OpenAI(\"gpt-3.5-turbo\",T=1.4,n = 3) # 3 completions per input with high temperature\n",
    "\n",
    "combine_answers = langtorch.Linear([[ f\"\\nAnswer {i}: \" for i in [1,2,3] ]]) # Here we use properties of matrix multiplication:\n",
    "# Linear uses matmul, where row_of_labels @ column_of_completions == one long entry with labeled completions\n",
    "\n",
    "chose = TextModule(\"Select from these reasoning paths the most consistent final answer: {}\")\n",
    "\n",
    "llm = langtorch.OpenAI(\"gpt-3.5-turbo\", T=0)\n",
    "\n",
    "self_consistent_calculate = torch.nn.Sequential(\n",
    "    calculate,\n",
    "    ensemble_llm,\n",
    "    combine_answers,\n",
    "    chose,\n",
    "    llm\n",
    ")"
   ],
   "metadata": {
    "id": "99321b6d36ed62e9",
    "ExecuteTime": {
     "end_time": "2024-06-05T14:42:17.712956600Z",
     "start_time": "2024-06-05T14:42:17.658257400Z"
    }
   },
   "id": "99321b6d36ed62e9"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "[The most consistent final answer is: 171 * 33 = 5643.]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = TextTensor(\"171*33\")\n",
    "self_consistent_calculate(input_tensor)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2c3c54753fc4475",
    "outputId": "6aace427-bfb8-4073-ef20-4acead091dde",
    "ExecuteTime": {
     "end_time": "2024-06-05T14:42:24.914980400Z",
     "start_time": "2024-06-05T14:42:17.996243300Z"
    }
   },
   "id": "c2c3c54753fc4475"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saving results from repeating these calls, let's us see accuracy increasing from 25% (using `calculate`) to well over 50% (using `self_consistent_calculate`) on this input.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "7618c5a1c833e23"
   },
   "id": "7618c5a1c833e23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Automatic TextTensor Embeddings for Building Retrievers\n",
    "\n",
    "`TextTensors` offer a straightforward way to work with embeddings. Every `TextTensor` can generate its own embeddings -- held in a torch tensor that preserves their shape. Moreover, `TextTensors` automatically act as their embeddings when passed to torch functions like cosine similarity.\n",
    "\n",
    "These representations (available under the `.embedding` attribute) are created automatically right before they are needed, using a set embedding model (default is OpenAI's `text-embedding-3-small`).\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "4c6720b07213067c"
   },
   "id": "4c6720b07213067c"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.6923, 0.6644, 0.6316, 0.5749],\n         [0.5459, 0.7728, 0.5386, 0.7037]]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor1 = TextTensor([[[\"Yes\"],\n",
    "                       [\"No\"]]])\n",
    "tensor2 = TextTensor([\"Yeah\", \"Nope\", \"Yup\", \"Non\"])\n",
    "\n",
    "torch.cosine_similarity(tensor1, tensor2)"
   ],
   "metadata": {
    "id": "8471c41a02c90cab",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3ab1337f-5186-4081-be3b-d833e49ef44a",
    "ExecuteTime": {
     "end_time": "2024-06-05T14:42:09.975874Z",
     "start_time": "2024-06-05T14:42:08.860708100Z"
    }
   },
   "id": "8471c41a02c90cab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can access the embedding tensor under `.embedding`, change the embedding model and embed using `.embed()`:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "8217b407f64f8d56"
   },
   "id": "8217b407f64f8d56"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[-0.0339,  0.0298, -0.0105,  ..., -0.0194, -0.0076,  0.0153]],\n\n         [[-0.0281,  0.0073, -0.0121,  ..., -0.0071,  0.0094,  0.0090]]]])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To change embedding model and embed\n",
    "tensor1.embedding_model = \"text-embedding-3-large\"\n",
    "tensor1.embed()\n",
    "# To access the embedding tensor\n",
    "tensor1.embedding"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4ea69f9a7274218",
    "outputId": "e6bbe437-6837-4f24-eba3-258560365a82",
    "ExecuteTime": {
     "end_time": "2024-06-05T14:42:10.648879900Z",
     "start_time": "2024-06-05T14:42:09.977875400Z"
    }
   },
   "id": "a4ea69f9a7274218"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working with embeddings and documents (parsing, chunking and indexing)\n",
    "\n",
    "To enable its functionalities `TextTensor` entries aren't just strings, but structured [`Text`](https://langtorch.org/reference/text) objects, which can be created from f-string templates, dictionaries and markup documents and are represented by a sequence of `(label, text)` pairs.\n",
    "\n",
    "For the next task we need chunked text data. We can use the above fact to conveniently manipulate markdown files -- in this example, a [paper on the abilities of language models](https://link.springer.com/article/10.1007/s11023-022-09602-0)."
   ],
   "metadata": {
    "collapsed": false,
    "id": "c1b5dc93a8cfe50f"
   },
   "id": "c1b5dc93a8cfe50f"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/adamsobieszek/langtorch/main/src/langtorch/conf/paper.md\n",
    "#     Download manually if wget unavailable"
   ],
   "metadata": {
    "id": "514259b30537e1a0",
    "ExecuteTime": {
     "end_time": "2024-06-05T14:43:26.460298400Z",
     "start_time": "2024-06-05T14:43:26.330907800Z"
    }
   },
   "id": "514259b30537e1a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can create a tensor with each markdown block in a separate entry simply with:"
   ],
   "metadata": {
    "id": "H96UlD5NZ3hl"
   },
   "id": "H96UlD5NZ3hl"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[# Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models\n",
      "  Adam Sobieszek & Tadeusz Price, 2022                                           \n",
      "  ## Abstract                                                                    ] \n",
      "  (...)\n",
      "shape = torch.Size([80])\n"
     ]
    }
   ],
   "source": [
    "paper = TextTensor.from_file(\"paper.md\")\n",
    "print(paper[:3],\"\\n  (...)\")\n",
    "print(f\"shape = {paper.shape}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71fc197b330f15cb",
    "outputId": "da4f37a0-8788-4938-bfe2-a2258d43abe9",
    "ExecuteTime": {
     "end_time": "2024-06-05T14:45:28.320305500Z",
     "start_time": "2024-06-05T14:45:26.570136Z"
    }
   },
   "id": "71fc197b330f15cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the text has headers and other text blocks, we need to extract only paragraphs. This is where text entries being structured becomes useful, as LangTorch provides `iloc` and `loc` accessors for `Text` entries and tensors:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "2ad5177a504b4ed3"
   },
   "id": "2ad5177a504b4ed3"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[This article contributes to the debate a...\n",
      "  These are questions put to Salvador Dali...\n",
      "  We take issue with some methodological u...\n",
      "  We’ll show some situations in which reve...\n",
      "  In the second part of this paper, we pro...\n",
      "  In their paper, Floridi and Chiriatti pr...\n",
      "  The logic of Floridi and Chiriatti’s rev...\n",
      "  The most mature theory quantifying such ...\n",
      "  How does thinking of answers as being in...\n",
      "  The absolute size of the change in proba...\n",
      "  The mathematically minded reader may equ...\n",
      "  That’s it for established theory. Before...\n",
      "  When trying to reject the null hypothesi...\n",
      "  The theory of informative questions can ...\n",
      "  GPT-3 is not a chatbot, but a general-pu...\n",
      "  Tree of possible continuations. Top: the...\n",
      "  Knowing this, we can compare a naïve met...\n",
      "  Recall that we are restricted in our met...\n",
      "  The effect that specifying the prompt be...\n",
      "  A person wanting to Turing-test GPT-3 wo...\n",
      "  Let’s start with the simple case of bina...\n",
      "  Mathematical questions, short of being i...\n",
      "  We can now see that informativity is not...\n",
      "  The concept of informativity has given u...\n",
      "  Thus, a discussion of the limits should ...\n",
      "  Is the previous analysis enough to infer...\n",
      "  In the following section, we will create...\n",
      "  Searle argued, that computers ''have syn...\n",
      "  Recall how a language model during train...\n",
      "  First, GPT-3 can continue a prompt in th...\n",
      "  Second, GPT can also translate between n...\n",
      "  The described regularity that underlies ...\n",
      "  It seems that Peregrin’s diagnosis of th...\n",
      "  One kind of language game GPT-3 can be s...\n",
      "  We need to move from explaining the unde...\n",
      "  The metaphor of activation spreading thr...\n",
      "  What are the limits of our ability to us...\n",
      "  Recall, from our discussion of psychomet...\n",
      "  A bad priming of a question, like \"When ...\n",
      "  To this end we’ll define games (as in \"g...\n",
      "  An example of a game, and perhaps GPT-3’...\n",
      "  This outline encapsulates the schema tha...\n",
      "  We’ve seen that GPT-3 can complete tasks...\n",
      "  Many different abilities have been propo...\n",
      "  Let us adopt the version of the Turing T...\n",
      "  So is the Truing test a game? First it m...\n",
      "  Let’s suppose that the Truing game satis...\n",
      "  To see whether GPT can be primed for tru...\n",
      "  If then there are no distributional clue...\n",
      "  The modal limit – GPT cannot produce the...\n",
      "  One remedy future large language model e...\n",
      "  Recall Leibniz’s machine (Leibniz, 1666)...\n",
      "  As Floridi & Chiriatti (2020) note, the ...\n",
      "  The British politician John Prescott was...\n",
      "  How should we expect our journalist to r...\n",
      "  A popular explanation is to claim (after...\n",
      "  The role of plausibility checking is to ...\n",
      "  The role of vigilance towards the source...\n",
      "  Evolutionarily these evolved mechanisms ...\n",
      "  We set out in this article to learn more...\n",
      "  Footnote 1: Note that when we speak of r...\n",
      "  Footnote 2: To produce the illusion of G...\n",
      "  Footnote 3: While there is some debate a...\n",
      "  Footnote 4: Although, this by no means t...\n",
      "  Footnote 5: Although we have rejected th...]\n"
     ]
    }
   ],
   "source": [
    "# Select paragraphs\n",
    "paragraphs = paper.loc[\"Para\"]\n",
    "# Remove empty entries\n",
    "paragraphs = paragraphs[paragraphs!=\"\"]\n",
    "print(paragraphs[:].apply(lambda x: x[:40] + \"...\"))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "654eaec83e14fe79",
    "outputId": "26005f7d-d5ec-4b2c-e337-17a682b63bd8",
    "ExecuteTime": {
     "end_time": "2024-06-05T14:45:33.029040300Z",
     "start_time": "2024-06-05T14:45:32.860954400Z"
    }
   },
   "id": "654eaec83e14fe79"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build Custom Retriever and RAG modules\n",
    "\n",
    "For complex modules we can subclass `TextModule` and as in PyTorch define our own init and forward methods.\n",
    "\n",
    "Using how `TextTensors` can automatically act as a`Tensor` of its embeddings, we can very compactly implement e.g. a retriever, which for each entry in the input finds in parallel `k` entries with the highest cosine similarity among the documents it holds:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "55f9b5573924ac01"
   },
   "id": "55f9b5573924ac01"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "class Retriever(TextModule):\n",
    "    def __init__(self, documents: TextTensor):\n",
    "        super().__init__()\n",
    "        self.documents = TextTensor(documents).view(-1)\n",
    "\n",
    "    def forward(self, query: TextTensor, k: int = 5):\n",
    "        cos_sim = torch.cosine_similarity(self.documents, query)\n",
    "        return self.documents[cos_sim.topk(k)]"
   ],
   "metadata": {
    "id": "7aa72db3e92792c",
    "ExecuteTime": {
     "end_time": "2024-06-05T14:45:34.333112600Z",
     "start_time": "2024-06-05T14:45:34.134676400Z"
    }
   },
   "id": "7aa72db3e92792c"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "[Recall how a language model during training must compress an untenable number of conditional probabilities. The only wa\n  to do this successfully is to pick up on the regularities in language (as pioneered by Shannon 1948). Why do we claim   \n  that learning to predict words, as GPT does, can be treated as compressing some information? Let’s assume we’ve         \n  calculated the conditional probability distribution given only the previous word of all English words. Consider, that   \n  such a language model can either be used as a (Markavion) language generator or, following Shannon, be used for an      \n  efficient compression of English texts. Continuing this duality, it has been shown, that if a language model such as GPT\n  would be perfectly trained it can be used to optimally compress any English text (using arithmetic coding on its        \n  predicted probabilities; Shmilovici et al., 2009). Thus the relationship between prediction and compression is that     \n  training a language generator is equivalent to training a compressor, and a compressor must know something about the    \n  regularities present in its domain (as formalized in AIXI theory; Mahoney 2006). To make good predictions it is not     \n  enough to compress information about what words to use to remain grammatical (to have a syntactical capacity), but also \n  about all the regularities that ensure an internal coherence of a piece of text. Couldn’t it be feasible that among     \n  these GPT has picked up on regularities that go beyond syntax? We believe so, which we’ll illustrate with examples of   \n  how GPT can associate certain types of syntax with the content of the text, and even content to other content, which    \n  could imply that existing theories of syntax and semantics do not account well for its abilities.                       \n  An example of a game, and perhaps GPT-3’s most surprising ability, is its ability to write a computer program when     \n  prompted with a natural language description of it (Pal, 2021). In humans this skill, notwithstanding understanding the \n  description, requires the procedural ability of expression in a formal programming language. GPT excels at syntactical  \n  tasks semantically evoked, because the skill of permutation of symbols lends itself extremely well to compression. To   \n  understand what we may mean by compression (of a skill) we need to invoke Kolmogorov Complexity – a task is compressible\n  if correct answering can be simulated well with a short programme (low Kolmogorov Complexity) — a programme that is     \n  shorter than listing all solutions. A similar definition has been used in AIXI theory, where the size of the compressor \n  counts towards the size of a compressed file. In such easily-compressible tasks we claim that compression leads to      \n  generalisation — the ability to perform tasks seen in the training set on previously unseen inputs (as in Solomonoff’s  \n  induction, where shorter programmes lead to better predictions). This in turn creates the ability to operate on novel   \n  inputs and create novel outputs. GPT’s successes in such cases have even led to its evolution into OpenAI’s             \n  CodexFootnote3, where it has been shown not to just memorize solutions to such problems but generate novel solutions (  \n  contrary to early detractor’s accusations of \"mere copy and pasting\"), generalization being also a much better          \n  compression strategy. These ideas have been explicitly used in deep learning for example in the development of          \n  Variational Autoencoders, where compression drives the need to find the underlying features of data, and which endows   \n  these models with the ability to generate new examples (Kingma & Welling, 2019). In short: prediction leads to          \n  compression, compression leads to generalisation, and generalisation leads to computer intelligence.                    \n  The described regularity that underlies this ability is an example of what linguists and machine learning researchers  \n  call the distributional hypothesis (Boleda, 2020) - that semantic relationships present themselves as regularities or   \n  distributional patterns in language data. While we do not espouse a distributional theory of semantics – words being    \n  \"characterized by the company they keep\" (Firth, 1957), we nonetheless see empirical support for the fact that semantic \n  relationships can be learned from texts alone (for example in word embeddings, or through learning knowledge graphs, see\n  respectively Almeida & Xexéo 2019 and Nickel et al., 2015). Thus, in order to compress probabilities GPT learns         \n  regularities indiscriminately, semantic or otherwise, which endows it with the ability to predict semantically related  \n  continuations.                                                                                                          \n  This outline encapsulates the schema that can describe the spectacular successes of deep learning on many language     \n  tasks, execution of which is well simulated under such conditions (e.g. tasks requiring creative writing).Footnote4 What\n  is of interest to us is to think what tasks would not be well executed under such a scheme. The notion of game is what  \n  we’ll use to identify such tasks not only for GPT-3, but its successors, as the prediction of unlabelled text data seems\n  bound to be the pervasive paradigm of large language models in the foreseeable future. With this, we are finally ready  \n  to discuss a task which seems to lend itself poorly to compression – the Turing Test.                                   \n  We’ve seen that GPT-3 can complete tasks that require the use of semantic relationshipsFootnote5 (e.g. \"A shoe is to a \n  foot, as a hat is to what?\") and symbol manipulation (e.g. coding). However, not all tasks can be completed using just  \n  these abilities. We can now aim to find out whether the Turing Test is such a task. To say whether GPT can play the     \n  imitation game well, we need to explore whether the abilities required in the Turing Test are simulated well with       \n  compression of regularities and answer whether the Turing Test is even a game (in the sense that it exploits an existing\n  regularity that can be prompted)?                                                                                       ]"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = Retriever(paragraphs)\n",
    "query = TextTensor(\"What's the relationship between prediction and compression?\")\n",
    "retriever(query)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-05T14:45:37.222113500Z",
     "start_time": "2024-06-05T14:45:34.967083200Z"
    }
   },
   "id": "ebe8aaea232e5366"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note how the implementation didn't require us to learn about any new operations we would not find in regular PyTorch. One goal of LangTorch is to give developers control over these lower level operations, while being able to write compact code without a multitude of classes. For this reason implementations such as the retriever above are not pre-defined classes in the main package.  \n",
    "We can now compose this module with a Module making LLM calls to get a custom Retrieval Augmented Generation pipeline:\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "898a81d71b708804"
   },
   "id": "898a81d71b708804"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class RAG(TextModule):\n",
    "    def __init__(self, documents: TextTensor, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.retriever = Retriever(documents)\n",
    "\n",
    "    def forward(self, user_message: TextTensor, k: int = 5):\n",
    "        retrieved_context = self.retriever(user_message, k) + \"\\n\"\n",
    "        user_message = user_message + \"\\nCONTEXT:\\n\" + retrieved_context.sum()\n",
    "        return super().forward(user_message)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-05T14:46:12.497063100Z",
     "start_time": "2024-06-05T14:46:12.449401800Z"
    }
   },
   "id": "9710007db3ab0c65"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[The relationship between prediction and compression is that training a language generator, such as GPT, is equivalent to training a compres↵\n",
      "  sor. In order to successfully compress information and make accurate predictions, the model must pick up on the regularities present in the↵ \n",
      "   language. As the model learns to predict words, it is effectively compressing information by identifying patterns and regularities in the ↵ \n",
      "  text. This ability to compress information leads to generalization, allowing the model to operate on novel inputs and create novel outputs.↵ \n",
      "   In the context of GPT's success in tasks such as writing computer programs or completing semantic relationship tasks, the model's ability ↵ \n",
      "  to compress information plays a crucial role in its performance.                                                                            ]]\n"
     ]
    }
   ],
   "source": [
    "rag_chat = RAG(paragraphs,\n",
    "               prompt=\"Use the context to answer the following user query: \",\n",
    "               activation=\"gpt-3.5-turbo\")\n",
    "assistant_response = rag_chat(query)\n",
    "print(assistant_response.reshape(1,1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-05T14:46:16.459414500Z",
     "start_time": "2024-06-05T14:46:12.770318500Z"
    }
   },
   "id": "2db3ee83902af859"
  },
  {
   "cell_type": "markdown",
   "source": [
    "With only small modifications to the retriever this module could also perform batched inference — performing multiple simultaneous queries without much additional latency. Note, `prompt` and `activation` are arguments inherited from TextModule and need the `super().forward` call to work.\n",
    "We are excited to see what you will build with LangTorch. If you want to share some examples or have any questions, feel free to ask on our [discord](https://discord.gg/jkreqtCCkv). In the likely event of encountering a bug send it on discord or post on the [GitHub Repo](https://github.com/AdamSobieszek/langtorch) and we will fix it ASAP."
   ],
   "metadata": {
    "collapsed": false,
    "id": "2b0b68131396bd40"
   },
   "id": "2b0b68131396bd40"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
