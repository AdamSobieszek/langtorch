{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LangTorch\n",
    "\n",
    "To install LangTorch using pip:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "d0b2f2f0d1134b23"
   },
   "id": "d0b2f2f0d1134b23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Setup Colab\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Install LangTorch\n",
    "!pip install langtorch"
   ],
   "metadata": {
    "id": "22a61b1b1221f143",
    "collapsed": true
   },
   "id": "22a61b1b1221f143"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To use the OpenAI API as our LLM, we need to set the `OPENAI_API_KEY` environment variable. You can find your API key on platform.openai.com"
   ],
   "metadata": {
    "collapsed": false,
    "id": "4e001fb486dc11"
   },
   "id": "4e001fb486dc11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\"  # Replace with your actual OpenAI API key"
   ],
   "metadata": {
    "id": "e7ec525bcfd71e06"
   },
   "id": "e7ec525bcfd71e06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Perform multiple LLM calls with TextTensors"
   ],
   "metadata": {
    "collapsed": false,
    "id": "f28f0fe0532da2ad"
   },
   "id": "f28f0fe0532da2ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langtorch import TextTensor  # holds texts instead of weights, supports tensor operations\n",
    "from langtorch import TextModule  # torch.nn modules working on TextTensors, perform prompt templating and llm calls"
   ],
   "metadata": {
    "id": "3dff6245c28f56c7"
   },
   "id": "3dff6245c28f56c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**`TextTensors`** are designed to streamline working with many pieces of text and performing parallel LLM calls. `langtorch.TextTensor` is a subclass of PyTorch's `torch.Tensor` that:\n",
    "\n",
    "- **Holds text entries** instead of numerical weights.\n",
    "- **Special Structure:** `TextTensors` entries can represent chunked documents, prompt templates, completion dictionaries, chat histories, and more.\n",
    "- **Represents Geometrically:** `TextTensors` have a shape and can be modified with PyTorch functions (reshape, stack, etc.).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In this example, we will create tensors holding prompt templates, fill them with a tensor of completion dictionaries, and send them to the OpenAI API.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "39dcc0595a6a376d"
   },
   "id": "39dcc0595a6a376d"
  },
  {
   "cell_type": "code",
   "source": [
    "prompt_tensor = TextTensor([[\"Is this an email address? {input_field}\"],\n",
    "                            [\"Is this a valid web link? {input_field}\"]])\n",
    "\n",
    "# Adding TextTensors appends their content according to broadcasting rules\n",
    "prompt_tensor += \" (Answer 'Yes' or 'No')\"\n",
    "print(prompt_tensor)\n",
    "print(\"Shape =\", prompt_tensor.shape)"
   ],
   "metadata": {
    "id": "PSYLm9qT4PGj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "78676ca0-7a9e-4b0e-af17-6be0b6378ae8"
   },
   "id": "PSYLm9qT4PGj",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "`TextModules` are `torch.nn.Modules` that work on `TextTensors`:\n",
    "\n",
    "- **Tensor of Prompts:** They hold a tensor of prompts instead of numerical weights.\n",
    "- **Input Handling:** They accept `TextTensors` as input, which are used to format the prompt tensor.\n",
    "- **Formatting and Broadcasting:** This allows formatting multiple prompts on multiple completions, controlling which prompt gets which input through broadcasting rules.\n",
    "- **Activation Function:** Most torch layers end with an *activation function*. Similarly, `TextModules` end in an *activation* of an LLM call.\n",
    "\n",
    "In this example, we will create a `TextModule` that ends in a call to an OpenAI model. **This module can now execute both tasks in parallel on as many inputs as we'd like:**\n"
   ],
   "metadata": {
    "id": "YtZDQxJ89xFC"
   },
   "id": "YtZDQxJ89xFC"
  },
  {
   "cell_type": "code",
   "source": [
    "tasks_module = TextModule(prompt_tensor, activation=\"gpt-3.5-turbo\")\n",
    "\n",
    "input_completions = TextTensor([{\"input_field\": \"contact@langtorch.org\"}, {\"input_field\": \"https://langtorch.org\" }])\n",
    "\n",
    "# The first row of the output are answers to \"Is this an email address?\", second to \"Is this a valid web link?\"\n",
    "# Columns are the two input completions\n",
    "print(tasks_module(input_completions))"
   ],
   "metadata": {
    "id": "jHwDZ4GX46hK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7f781a72-1e53-444e-ebbf-325b6df1e8c1"
   },
   "id": "jHwDZ4GX46hK",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparison with the OpenAI Package\n",
    "\n",
    "The `TextModule` above both formats the prompts and sends them to the OpenAI activation (`langtorch.OpenAI`). Let's compare LangTorch to the OpenAI package.\n",
    "\n",
    "First, we'll separate the formatting and API steps.\n",
    "\n",
    "> A core feature of `TextTensors` is that they allow us to easily format several prompts on several inputs.\n",
    ">\n",
    "> LangTorch achieves this by **defining the product** of two `TextTensors`: `text1*text2` as an operation akin to `text1.format(**text2)`. As shown below this is what happens in a `TextModule` before adding an activation:\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "5de346885d33eb67"
   },
   "id": "5de346885d33eb67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using TextModule\n",
    "tasks_module = TextModule(prompt_tensor)\n",
    "prompts = tasks_module(input_completions)\n",
    "\n",
    "# Equivalently, using \"TextTensor multiplication\"\n",
    "prompts = prompt_tensor*input_completions\n",
    "print(prompts)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1bd9231ac309e8c",
    "outputId": "772732d5-1b40-48b3-9a2f-f42c71631888"
   },
   "id": "c1bd9231ac309e8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    " The code above introduces the multiplication operation (used in TextModules), which acts like a more powerful format operation and allows for the various features of TextTensors. For a more in depth look, see [TextTensor Multiplication](langtorch.org/reference/multiplication).\n",
    "\n",
    "We can send the formatted prompts to the OpenAI API by creating a `langtorch.OpenAI` module (the \"activation\") and compare speed between three API use cases:\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "cc6f82ff60c73d4a"
   },
   "id": "cc6f82ff60c73d4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openai\n",
    "import langtorch\n",
    "import time\n",
    "\n",
    "langtorch_api = langtorch.OpenAI(\"gpt-3.5-turbo\", system_message=\"You are a helpful assistant.\", max_token=1, T=0.)\n",
    "openai_api = openai.OpenAI()\n",
    "\n",
    "# Open AI package\n",
    "start = time.time()\n",
    "responses = []\n",
    "for prompt in prompts.flat:\n",
    "    responses.append(openai_api.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=1,\n",
    "        temperature=0.\n",
    "    ))\n",
    "print(f\"1.\\n{str(responses)[:125]}...\")\n",
    "print(f\" OpenAI loop time taken: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# LangTorch\n",
    "start = time.time()\n",
    "responses = langtorch_api(prompts)\n",
    "print(f\"2.\\n{responses}\")\n",
    "print(f\" LangTorch time taken: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# LangTorch on repeated requests\n",
    "start = time.time()\n",
    "responses = langtorch_api(prompts)\n",
    "print(f\"3.\\n{responses}\")\n",
    "print(f\" LangTorch on repeated requests time taken: {time.time() - start:.2f} seconds\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90f08316c82c7e11",
    "outputId": "2bf36d2f-0843-4172-e464-1ae84c72f39b"
   },
   "id": "90f08316c82c7e11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "The OpenAI Activation in LangTorch (`langtorch.OpenAI`) isn't just a wrapper around the OpenAI package. The observed speed up comes from the fact that the LangTorch implementation:\n",
    "\n",
    "- Sends API calls in parallel, allowing multiple completions to be generated much faster than calling the OpenAI chat completion endpoint in sequence.\n",
    "- Saves on tokens and speeds up subsequent calls by caching API results, especially for embeddings and when the temperature is set to zero.\n",
    "- Optimizes requested API calls removing duplicates\n",
    "\n",
    "`langtorch.OpenAI` also:\n",
    "\n",
    "- Operates directly on `TextTensors`, returning calls in the same shape as the input.\n",
    "- Handles API errors and retries by default"
   ],
   "metadata": {
    "collapsed": false,
    "id": "fcd90c1e3545e667"
   },
   "id": "fcd90c1e3545e667"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Implementing popular methods\n",
    "\n",
    "### Chained Calls and Simple Zero-Shot Chain of Thought\n",
    "\n",
    "LangTorch integrates seamlessly with torch, allowing you to easily chain `TextModules` using `torch.nn.Sequential`. This can be used to chain multiple LLM calls or additional prompting methods. A simplified example is a zero-shot Chain of Thought, for which we can create a reusable `TextModule`:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "de2f3749a8a6dbbf"
   },
   "id": "de2f3749a8a6dbbf"
  },
  {
   "cell_type": "code",
   "source": [
    "CoT = TextModule(\"{*} Let's think step by step.\")"
   ],
   "metadata": {
    "id": "ja-yEF8xY9G7"
   },
   "id": "ja-yEF8xY9G7",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "`{}` in a prompt template is a positional argument, taking one input argument in each entry. For our chain of thought module we use the placeholder `{*}`, which is a \"wildcard\" key that places all the input entries in its place.\n",
    "\n",
    "Now to chain these `torch.nn.Sequential`:"
   ],
   "metadata": {
    "id": "u22Fx-UXZQV1"
   },
   "id": "u22Fx-UXZQV1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "calculate = TextModule(\"Calculate the following: {} = ?\")\n",
    "\n",
    "calculate_w_CoT = torch.nn.Sequential(\n",
    "    calculate,\n",
    "    CoT,\n",
    "    langtorch.OpenAI(\"gpt-3.5-turbo\") ,\n",
    "    # You can add sequential calls here\n",
    ")\n",
    "\n",
    "input_tensor = TextTensor([\"170*32\", \"123*45/10\", \"2**10*5\"])\n",
    "output_tensor = calculate_w_CoT(input_tensor)\n",
    "output_tensor.view(1,-1) # We use torch methods to reshape TextTensors and view entries in columns"
   ],
   "metadata": {
    "id": "ae1d705bce57772b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e889b758-ab47-4dc9-adc1-1138fa671d92"
   },
   "id": "ae1d705bce57772b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ensemble / Self-Consistency\n",
    "\n",
    "Representing texts geometrically in a matrix or tensor allows for creating meaningful structures. Methods like ensemble voting and self-consistency involve generating multiple completions for the same task, easily represented by adding a dimension.\n",
    "\n",
    "In this example, we build a module that creates multiple Chain-of-Thought answers for each input. These create separate `TextTensor` entries that we combine using a \"linear layer\" to marginalize over them, improving overall performance (see [Wang et al., 2022](https://arxiv.org/abs/2203.11171)).\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "4ea191cb538d863e"
   },
   "id": "4ea191cb538d863e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "calculate = TextModule(\"Calculate the following: {} = ? Let's think step by step.\")\n",
    "\n",
    "ensemble_llm = langtorch.OpenAI(\"gpt-3.5-turbo\",T=1.4,n = 3) # 3 completions per input with high temperature\n",
    "\n",
    "combine_answers = langtorch.Linear([[ f\"\\nAnswer {i}: \" for i in [1,2,3] ]]) # Here we use properties of matrix multiplication:\n",
    "# Linear uses matmul, where row_of_labels @ column_of_completions == one long entry with labeled completions\n",
    "\n",
    "chose = TextModule(\"Select from these reasoning paths the most consistent final answer: {}\")\n",
    "\n",
    "llm = langtorch.OpenAI(\"gpt-3.5-turbo\", T=0)\n",
    "\n",
    "self_consistent_calculate = torch.nn.Sequential(\n",
    "    calculate,\n",
    "    ensemble_llm,\n",
    "    combine_answers,\n",
    "    chose,\n",
    "    llm\n",
    ")"
   ],
   "metadata": {
    "id": "99321b6d36ed62e9"
   },
   "id": "99321b6d36ed62e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_tensor = TextTensor(\"171*33\")\n",
    "self_consistent_calculate(input_tensor)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2c3c54753fc4475",
    "outputId": "6aace427-bfb8-4073-ef20-4acead091dde"
   },
   "id": "c2c3c54753fc4475"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saving results from repeating these calls, let's us see accuracy increasing from 25% (using `calculate`) to well over 50% (using `self_consistent_calculate`) on this input.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "7618c5a1c833e23"
   },
   "id": "7618c5a1c833e23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Automatic TextTensor Embeddings for Building Retrievers\n",
    "\n",
    "`TextTensors` offer a straightforward way to work with embeddings. Every `TextTensor` can generate its own embeddings -- held in a torch tensor that preserves their shape. Moreover, `TextTensors` automatically act as their embeddings when passed to torch functions like cosine similarity.\n",
    "\n",
    "These representations (available under the `.embedding` attribute) are created automatically right before they are needed, using a set embedding model (default is OpenAI's `text-embedding-3-small`).\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "4c6720b07213067c"
   },
   "id": "4c6720b07213067c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tensor1 = TextTensor([[[\"Yes\"],\n",
    "                       [\"No\"]]])\n",
    "tensor2 = TextTensor([\"Yeah\", \"Nope\", \"Yup\", \"Non\"])\n",
    "\n",
    "torch.cosine_similarity(tensor1, tensor2)"
   ],
   "metadata": {
    "id": "8471c41a02c90cab",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3ab1337f-5186-4081-be3b-d833e49ef44a"
   },
   "id": "8471c41a02c90cab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can access the embedding tensor under `.embedding`, change the embedding model and embed using `.embed()`:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "8217b407f64f8d56"
   },
   "id": "8217b407f64f8d56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To change embedding model and embed\n",
    "tensor1.embedding_model = \"text-embedding-3-large\"\n",
    "tensor1.embed()\n",
    "# To access the embedding tensor\n",
    "tensor1.embedding"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4ea69f9a7274218",
    "outputId": "e6bbe437-6837-4f24-eba3-258560365a82"
   },
   "id": "a4ea69f9a7274218"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working with embeddings and documents (parsing, chunking and indexing)\n",
    "\n",
    "To enable its functionalities `TextTensor` entries aren't just strings, but structured [`Text`](https://langtorch.org/reference/text) objects, which can be created from f-string templates, dictionaries and markup documents and are represented by a sequence of `(label, text)` pairs.\n",
    "\n",
    "For the next task we need chunked text data. We can use the above fact to convenietly manipulate markdown files -- in this example, a [paper on the abilities of language models](https://link.springer.com/article/10.1007/s11023-022-09602-0)."
   ],
   "metadata": {
    "collapsed": false,
    "id": "c1b5dc93a8cfe50f"
   },
   "id": "c1b5dc93a8cfe50f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://raw.githubusercontent.com/adamsobieszek/langtorch/main/src/langtorch/conf/paper.md"
   ],
   "metadata": {
    "id": "514259b30537e1a0"
   },
   "id": "514259b30537e1a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can create a tensor with each markdown block in a seperate entry simply with:"
   ],
   "metadata": {
    "id": "H96UlD5NZ3hl"
   },
   "id": "H96UlD5NZ3hl"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "paper = TextTensor.from_file(\"/content/paper.md\")\n",
    "print(paper[:3],\"\\n  (...)\")\n",
    "print(f\"shape = {paper.shape}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71fc197b330f15cb",
    "outputId": "da4f37a0-8788-4938-bfe2-a2258d43abe9"
   },
   "id": "71fc197b330f15cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the text has headers and other text blocks, we need to extract only paragraphs. This is where text entries being structured becomes useful, as LangTorch provides `iloc` and `loc` accessors for `Text` entries and tensors:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "2ad5177a504b4ed3"
   },
   "id": "2ad5177a504b4ed3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select paragraphs\n",
    "paragraphs = paper.loc[\"Para\"]\n",
    "# Remove empty entries\n",
    "paragraphs = paragraphs[paragraphs!=\"\"]\n",
    "print(paragraphs[:].apply(lambda x: x[:40] + \"...\"))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "654eaec83e14fe79",
    "outputId": "26005f7d-d5ec-4b2c-e337-17a682b63bd8"
   },
   "id": "654eaec83e14fe79"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build Custom Retriever and RAG modules\n",
    "\n",
    "For complex modules we can subclass `TextModule` and as in PyTorch define our own init and forward methods.\n",
    "\n",
    "Using how `TextTensors` can automatically act as a`Tensor` of its embeddings, we can very compactly implement e.g. a retriever, which for each entry in the input finds in parallel `k` entries with the highest cosine similarity among the documents it holds:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "55f9b5573924ac01"
   },
   "id": "55f9b5573924ac01"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Retriever(TextModule):\n",
    "    def __init__(self, documents: TextTensor):\n",
    "        super().__init__()\n",
    "        self.documents = TextTensor(documents).view(-1)\n",
    "\n",
    "    def forward(self, query: TextTensor, k: int = 5):\n",
    "        cos_sim = torch.cosine_similarity(self.documents, query)\n",
    "        return self.documents[cos_sim.topk(k)]"
   ],
   "metadata": {
    "id": "7aa72db3e92792c"
   },
   "id": "7aa72db3e92792c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retriever = Retriever(paragraphs)\n",
    "query = TextTensor(\"What's the relationship between prediction and compression?\")\n",
    "retriever(query)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebe8aaea232e5366"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note how the implementation didn't require us to learn about any new operations we would not find in regular PyTorch. One goal of LangTorch is to give developers control over these lower level operations, while being able to write compact code without a multitude of classes. For this reason implementations such as the retriever above are not pre-defined classes in the main package.  \n",
    "We can now compose this module with a Module making LLM calls to get a custom Retrieval Augmented Generation pipeline:\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "898a81d71b708804"
   },
   "id": "898a81d71b708804"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RAG(TextModule):\n",
    "    def __init__(self, documents: TextTensor, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.retriever = Retriever(documents)\n",
    "\n",
    "    def forward(self, user_message: TextTensor, k: int = 5):\n",
    "        retrieved_context = self.retriever(user_message, k) + \"\\n\"\n",
    "        user_message = user_message + \"\\nCONTEXT:\\n\" + retrieved_context.sum()\n",
    "        return super().forward(user_message)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9710007db3ab0c65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rag_chat = RAG(paragraphs,\n",
    "               prompt=\"Use the context to answer the following user query: \",\n",
    "               activation=\"gpt-3.5-turbo\")\n",
    "assistant_response = rag_chat(query)\n",
    "print(assistant_response.reshape(1,1))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2db3ee83902af859"
  },
  {
   "cell_type": "markdown",
   "source": [
    "With only small modifications to the retriever this module could also perform batched inference — performing multiple simultaneous queries without much additional latency. Note, `prompt` and `activation` are arguments inherited from TextModule and need the `super().forward` call to work.\n",
    "We are excited to see what you will build with LangTorch. If you want to share some examples or have any questions, feel free to ask on our [discord](https://discord.gg/jkreqtCCkv). In the likely event of encountering a bug send it on discord or post on the [GitHub Repo](https://github.com/AdamSobieszek/langtorch) and we will fix it ASAP."
   ],
   "metadata": {
    "collapsed": false,
    "id": "2b0b68131396bd40"
   },
   "id": "2b0b68131396bd40"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
